{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments from the 'Emerging Structures in Computational Graphs of Neural Networks' project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Imports and Setups\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set up the environment for remote notebook execution & check node configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# access_token = \"Your Access Token for Gemma\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"./NetworkStructures/\") # /!\\ Comment out if \".\" is not home directory in goethe's cluster. To check use : print(os.getcwd())\n",
    "\n",
    "print(os.environ.get(\"HOSTNAME\"))\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import project's modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight.models.UnifiedTransformer import UnifiedTransformer\n",
    "from connectivity.effective import get_circuit_feature\n",
    "from evaluation.faithfulness import faithfulness as faithfulness_fn\n",
    "from data.buffer import unpack_batch, ioi_buffer, simple_rc_buffer, rc_buffer, single_input_buffer\n",
    "\n",
    "from utils.ablation_fns import id_ablation\n",
    "from utils.plotting import plot_faithfulness\n",
    "from utils.metric_fns import metric_fn_logit, metric_fn_KL, metric_fn_statistical_distance\n",
    "from utils.experiments_setup import load_model_and_modules, load_saes, get_architectural_graph\n",
    "from utils.activation import get_hidden_states, get_is_tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the multi gpu setup is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the SAEs are used properly\n",
    "\n",
    "Check L1 and variance explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if studied models are able to solve the tasks.\n",
    "\n",
    "If not, there is no point in trying to find out how it solves it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model_name in [\"gemma-2-2b\", \"pythia-70m-deduped\", \"gpt2\"]:\n",
    "#     print(\"##########\")\n",
    "#     print(model_name)\n",
    "#     print(\"##########\")\n",
    "#     # model, name2mod = load_model_and_modules(device=DEVICE, model_name=model_name, resid=use_resid, attn=use_attn_mlp, mlp=use_attn_mlp, start_at_layer=start_at_layer)\n",
    "#     model = UnifiedTransformer(\n",
    "#         model_name,\n",
    "#         device=DEVICE,\n",
    "#         use_auth_token=access_token,        \n",
    "#     )\n",
    "#     with torch.no_grad():\n",
    "#         model.device = model.cfg.device\n",
    "#         model.tokenizer.padding_side = 'left'\n",
    "#         for buffer_fn in [ioi_buffer, rc_buffer, simple_rc_buffer, gp_buffer, gt_buffer]:\n",
    "#             perm=torch.randperm(400)\n",
    "#             buffer = buffer_fn(model, 1, DEVICE, perm=perm)\n",
    "#             all_metrics = []\n",
    "#             c = 0\n",
    "#             for batch in tqdm(buffer):\n",
    "#                 tokens, trg_idx, trg, corr, corr_trg = unpack_batch(batch)\n",
    "#                 c += 1\n",
    "#                 with model.trace(tokens):\n",
    "#                     metric_kwargs = {\"trg_idx\": trg_idx, \"trg_pos\": trg, \"trg_neg\": corr_trg}\n",
    "#                     all_metrics.append(metric_fn_logit(model, metric_kwargs).save())\n",
    "#             try:\n",
    "#                 all_metrics = torch.stack(all_metrics)\n",
    "#                 mean_logit = all_metrics.mean().item()\n",
    "#                 accuracy = (all_metrics > 0).float().mean().item()\n",
    "#                 print(f'Buffer {buffer_fn.__name__} done, {c} batches processed')\n",
    "#                 print(f\"Buffer {buffer_fn.__name__} mean logit: {mean_logit} accuracy: {accuracy}\")\n",
    "#             except RuntimeError:\n",
    "#                 print(f\"Buffer {buffer_fn.__name__} failed with {c} batches processed\")\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Task            | Pythia-70m-deduped | Pythia-70m-deduped | GPT-2 | GPT-2 | Gemma-2-2b | Gemma-2-2b |\n",
    "|-------------------|--------------------------|------------------------|-------------|-----------|------------------|----------------|\n",
    "|                   | Mean Logit | Accuracy     | Mean Logit | Accuracy   | Mean Logit | Accuracy  |\n",
    "| IOI        | -0.063     | 0.0          | 1.188      | 1.0        | 14.1      | 1.0       |\n",
    "| SV-agreement         | 1.940      | 0.995        | -          | -          | 10.3      | 0.99      |\n",
    "| Simple SV-agreement  | 3.991      | 1.0          | 4.530      | 1.0        | 16.5      | 1.0       |\n",
    "| Gender-Pronoun         | 0.978      | 0.755        | 2.842      | 0.907      | 12.9      | 0.90      |\n",
    "| Greater Than         | 2.911      | 0.817        | 2.951      | 1.0        | NaN       | NaN       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Circuit Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define a model to be disected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_attn_mlp = False\n",
    "use_resid = True\n",
    "start_at_layer = 2\n",
    "model_name = \"gemma-2-2b\"\n",
    "model, name2mod = load_model_and_modules(device=DEVICE, model_name=model_name, resid=use_resid, attn=use_attn_mlp, mlp=use_attn_mlp, start_at_layer=start_at_layer)\n",
    "architectural_graph = get_architectural_graph(model, name2mod)\n",
    "\n",
    "dictionaries = load_saes(model, name2mod)\n",
    "print(architectural_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define a task to be solved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = simple_rc_buffer(model, 1, DEVICE, ctx_len=None, perm=None)\n",
    "batch = next(buffer)\n",
    "tokens, trg_idx, trg, corr, corr_trg = unpack_batch(batch)\n",
    "\n",
    "clean = tokens\n",
    "patch = corr\n",
    "\n",
    "metric_fn = metric_fn_logit\n",
    "metric_fn_dict = {\n",
    "    'logit': metric_fn_logit,\n",
    "    'KL': metric_fn_KL,\n",
    "    'Statistical Distance': metric_fn_statistical_distance,\n",
    "}\n",
    "\n",
    "metric_kwargs = {\"trg_idx\": trg_idx, \"trg_pos\": trg, \"trg_neg\": corr_trg}\n",
    "\n",
    "steps = 10\n",
    "edge_threshold = 1e-4\n",
    "edge_circuit = True\n",
    "\n",
    "default_ablation = 'id'\n",
    "ablation_fn = id_ablation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find the circuit that solves the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = get_circuit_feature(\n",
    "    clean=clean,\n",
    "    patch=patch,\n",
    "    model=model,\n",
    "    architectural_graph=architectural_graph,\n",
    "    name2mod=name2mod,\n",
    "    dictionaries=dictionaries,\n",
    "    metric_fn=metric_fn,\n",
    "    metric_kwargs=metric_kwargs,\n",
    "    ablation_fn=ablation_fn,\n",
    "    threshold=edge_threshold,\n",
    "    steps=steps,\n",
    "    edge_circuit=edge_circuit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate the quality of the circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_eval_thresholds = 20\n",
    "\n",
    "thresholds = torch.logspace(math.log10(edge_threshold), 0., nb_eval_thresholds, 10).tolist() # the higher the threshold, the more edges are removed. -1 is to enforce full ablation.\n",
    "\n",
    "results = faithfulness_fn(\n",
    "    model,\n",
    "    name2mod,\n",
    "    dictionaries,\n",
    "    clean,\n",
    "    edges,\n",
    "    architectural_graph,\n",
    "    thresholds,\n",
    "    metric_fn_dict,\n",
    "    metric_kwargs,\n",
    "    patch,\n",
    "    ablation_fn,\n",
    "    default_ablation=default_ablation,\n",
    "    node_ablation=(not edge_circuit),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_faithfulness(results, save_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError(\"Stop here.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archived cells\n",
    "\n",
    "Used for various tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Variance Explained, or whether the hidden states and the use of SAEs are correct\n",
    "\n",
    "# use_attn_mlp = False\n",
    "# use_resid = True\n",
    "# start_at_layer = 2\n",
    "# model_name = \"gpt2\"\n",
    "# model, name2mod = load_model_and_modules(device=DEVICE, model_name=model_name, resid=use_resid, attn=use_attn_mlp, mlp=use_attn_mlp, start_at_layer=start_at_layer)\n",
    "# architectural_graph = get_architectural_graph(model, name2mod)\n",
    "# dictionaries = load_saes(model, name2mod)\n",
    "\n",
    "# buffer = simple_rc_buffer(model, 1, DEVICE)\n",
    "# batch = next(buffer)\n",
    "# tokens, trg_idx, trg, corr, corr_trg = unpack_batch(batch)\n",
    "\n",
    "# visited = set()\n",
    "# to_visit = ['y']\n",
    "# while to_visit:\n",
    "#     downstream = to_visit.pop()\n",
    "#     if downstream in visited:\n",
    "#         continue\n",
    "#     visited.add(downstream)\n",
    "#     to_visit += architectural_graph[downstream]\n",
    "\n",
    "# all_submods = list(visited)\n",
    "# all_submods.remove('y')\n",
    "# all_submods = [name2mod[name] for name in all_submods]\n",
    "\n",
    "# is_tuple = get_is_tuple(model, all_submods)\n",
    "\n",
    "# hidden_states_clean = get_hidden_states(\n",
    "#     model, submods=all_submods, dictionaries=dictionaries, is_tuple=is_tuple, input=tokens\n",
    "# )\n",
    "\n",
    "# for k in hidden_states_clean:\n",
    "#     print(k, hidden_states_clean[k].act.shape, hidden_states_clean[k].res.shape) # should be (b, s, d_dict) and (b, s, d_model) respectively\n",
    "#     print(f\"L_0 : {(hidden_states_clean[k].act > 0.0).sum(dim=-1).float().mean()}\")\n",
    "#     print(f\"Error norm : {hidden_states_clean[k].res.norm(dim=-1).mean()}\")\n",
    "#     reconstructed = dictionaries[k].decode(hidden_states_clean[k].act) + hidden_states_clean[k].res\n",
    "#     print(f\"Original norm : {reconstructed.norm(dim=-1).mean()}\")\n",
    "#     print(f\"Variance Explained : {1 - ((hidden_states_clean[k].res).norm(dim=-1) / reconstructed.norm(dim=-1)).mean()}\")\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : check how to deal with multi gpu\n",
    "\n",
    "# from nnsight.models.UnifiedTransformer import UnifiedTransformer\n",
    "\n",
    "# model_name = \"pythia-70m-deduped\"\n",
    "# device = 'auto'\n",
    "\n",
    "# model = UnifiedTransformer(\n",
    "#         model_name,\n",
    "#         device_map=device,\n",
    "#         processing=False,\n",
    "#         n_devices=8\n",
    "#     )\n",
    "\n",
    "# print(model.cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
